



We are now trying to solve a FEM system using CG. More specifically,
we are trying to solve the following pde:
\[
-\Delta u = f \text{ in } \Omega
u = 0 \text{ on } \delta \Omega
\].

Idea of FEM is to find the projection of u, the exact solution, on a (finite dimensional) linear subspace V.
We look at the linear subspace of piecewise continous functions. Piecewise implies that the functions are
linear when restricted to some `piece', or in other words `element'. We will divide our domain into a set
of elements, for which we will then look at the space of piecewice continous functions.
One cannot divide the domain in an abritrare set of elements, as they must `glue' together a continous function.
The domain divions must satisfy the so-called `conformal' property to be a proper division for the piecewise
continous linear functions subspace.
In the rest of this paper we will look into triangulations of the domain, a division of the domain
in triangles. Such a triangulation is given by a set of triangles $T = \{T_1, \dots, T_n\}$ with
vertices $x_1, \dots, x_N$. 

A useful basis for the subspace V is the so-called `nodal' basis, which is uniquely determined by the following property:
\[
  \phi_i(x_j) = \delta_{ij} \text{ for vertices } x_j.
\]

The crux of FEM is that one can calculate the projection $u_V$ of $u$ on $V$ \emph{without} knowing the exact
solution. Write $u_V$ in the nodal basis,
\[
u_V = \alpha_1 \phi_1 + \dots + \alpha_m + \phi_m.
\]
Now $u_V$ can be determined as the solution of the equations
\[
(\alpha_1 \nabla \phi_1 + \dots + \alpha_m \nabla \phi_m,\nabla \phi_j) = (f, \phi_j) \text{ for } j \in \{1, \dots, m\},
\]
which translates to solving the system
\[
\begin{bmatrix}
            (\nabla\phi_1, \nabla\phi_1)& \cdots  & (\nabla\phi_m, \nabla\phi_m) \\
             \vdots         &  \ddots & \vdots \\
            (\nabla\phi_1,\nabla\phi_m) & \cdots  & (\nabla\phi_m, \nabla\phi_m)
           \end{bmatrix}
           \begin{bmatrix}
           \alpha_1 \\
           \vdots   \\
           \alpha_n
           \end{bmatrix}
           = \begin{bmatrix}
           (f,\phi_1) \\
           \vdots     \\
           (f,\phi_n)
 \end{bmatrix}.          
\]

This matrix is called the \emph{stiffness} matrix. The stiffness matrix is a sparse matrix, as nodal basis functions only `interact' with
a (small) number of other basis functions. Furthermore, it is a symmetric positive definite matrix because of the inner product.
FEM-applications mainly consist of two parts:
\begin{itemize}
\item Matrix assembly. A way to calculate the non-zero entries of a stiffness matrix.
\item Solving the FEM-system. A way to solve the system given above, because the matrices encoutered
      have a huge $n$ this is mainly done using iterative solvers, like \emph{CG}.
 \end{itemize}


\subsection{Matrix assembly}
How do we calculate the elements of the stiffness matrix; how do we calculate the inner proucts $(\phi_i', \phi_j')$?
This can be done in quite an elegant way, by writing this inner product as a sum of inner products over the triangles in our mesh:
\[
(\nabla\phi_i, \nabla\phi_j) = \int_{\Omega} \nabla \phi_i \cdot \nabla \phi_j = \sum_{k=1}^n \int_{T_k} \nabla \phi_i \cdot \nabla \phi_j = \sum_{k=1}^n (\phi_i, \phi_j)_{T_k}
\]

We see that the inner product over the entire domain reduces to a sum of `local' inner products, defined over a triangle in the
domain. This reduces the problem to finding these `local' element matrices, containing the inner products of basis functions
interacting on a certain triangle. Note that these `local' element matrices only have $3\times3$ non-zero elements,
as there are only 3 nodal basis functions interacting on this triangle. Now the local inner products only depend on the \emph{triangle} itself, as $\nabla \phi_i$ is a
constant function. One can proof that the $3\times3$ non-zero elements are given by:
BLABLBABLA

WE FIRST CONSTRUCT THE `ENTIRE' MATRIX WITH NODAL FUNCTIONS LIVING ON THE BOUNDARY AS WELL. EASIER TO USE LATER.
\subsection{Parallel}
In the previous sections we saw how to assembly the FEM-matrix. One could now use this approach to create a `naive' 
parallel FEM-solver: Let one processor assemble the stiffness matrix, find a parallel distribution for this matrix apply the
parallel CG from section X.

This however has two main disadvantages.
\begin{itemize}
\item The matrix assembly is an expensive step. We did not benefit from making this step parallel.
\item The relation between the geometrical object, the triangulation, and elements from the stiffness matrix is lost. 
      What do we mean by this? The element A_{ij} is non-zero iff the nodal basis $\phi_i$ interacts with $\phi_j$, which
			in turn only happens if the vertices $x_i$ and $x_j$ are connected by and edge in the triangulation. 
\end{itemize}


We opt another approach which defies these disadvantages. Remember that we could write 
\begin{equation}
A_{ij} = (\nabla phi_i, \nabla phi_j) = \sum_{k=1}^n (\nabla \phi_i, \nabla \phi_j)_{T_k}.
\label{eq:div}
\end{equation}
Instead of distributing the matrix elements $A_{ij}$, we can also distribute the triangles $T_k$ over p processors.
 This will divide our triangulation in $p$ (disjoint) sets
\[
T = T^1 \sqcup \dots \sqcup T^p = \{T^1_1, \dots, T^1_{n_1}\} \sqcup \{T^p_1, \dots, T^p_{n_p} \}.
\]
and leads to a partition of the domain in
\[
\Omega = \Omega^1 \cup \dots \cup \Omega^p.
\]
Also, using \eqref{eq:div} this leads to a natural decomposition of $A$,
\[
A = \sum_{k=1}^p A_k.
\]

This (directly) gives us a way to parallelize the matrix assembly step: every processor forms $A_k$, which is
then communicated to form the stifness matrix. Of course, we will not explicitely form the matrix as we have the 
following (trivial) identity:
\[
v = Au = \sum{k=1}^p (A_k u)
\] 
So a possible algorithm to calculate $Au$ is to first calculate $A_ku$ on each processor and then 
communicate the non-zeros to find $v$. 


If we look  closer at this algorithm then we can directly relate the amount of work and communication to the
distribution of triangles [EXAMPLE].
We have $v_i = (Au)_i = \sum_{j=1}^n A_{ij} u_j$, where a non-zero $A_{ij}$ implies that basis function $\phi_i$ interacts with $\phi_j$. 
Again looking at \eqref{eq:div}, we can deduce two situations:
\begin{itemize}
\item All the interactions with $\phi_i$ are contained by one processor $k$. In this case we have
      $A_{ij} = (A_k)_{ij}$ for $1 \leq k \leq n$. So $(Au)_i$ is completely determined by processor $k$ and the 
      matrix product $(A_k)u$. 
\item The interactions with $\phi_i$ are spread over a $n$ processors. In this case we have 
      $v_i = (Au)_i = (A_{k_1} u)_i + \dots + (A_{k_n} u)_i$. We need to communicate the $n$ local
			matrix vector results and sum these to find the final result.
\end{itemize}
Looking at the triangulation we see that the first case corresponds to the situation where vertex $x_i$ is only
used in triangles that are owned by one proccesor.  The second case corresponds to the situation where a vertex is used in triangles that are pocessed by different processors.

Here we see the second advantage of this approach. By distributing the triangles over the processors we can balance the 
computation and communication cost of a matrix-vector multiplication.

\subsection{Exact formulation}
We assume that we have distributed the triangles over the processors (mondriaan?). 

First we will look howto form $A_k$, the matrix corresponding to the local processor. This however
is just forming a FEM matrix for the reduced domain.

For calculating $Au$ we now have two options: 
\begin{itemize}
\item We calculate $(A_k u)$ on every processor, after which we communicate the values on the boundary
      of the local triangulation, so that every of these processors holds the correct value.
\item We assign every vertex on the boundary to a specific processor. This processor will then
      enlarge it.. BULLSHIT
\end{itemize}

We see that vertices shared on the boundary impose a problem, as multiple processors hold (the same) value
of the corresponding element in $v = Au$. To solve this, we make one processor the explicit owner of a vertex.
To find the total vector $v$ we can request the corresponding values from the owners of these values. 

\subsection{Implementation details}
We have a local numbering of the vertices (inside a processor) and a global numbering. If a vertex is shared by 
multiple processors, then we must exactly which other processors hold this vertex.  Processor 0 is responsible for
dividing the triangulation of the processors. We 














