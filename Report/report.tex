\documentclass[11pt]{amsart}

\usepackage{geometry}
\usepackage{showlabels}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{algpseudocode}
\usepackage{todonotes}
\usepackage[noline,ruled]{algorithm2e}
\usepackage{enumerate}
\usepackage{graphicx}
%\setlength{\parindent}{0pt}
\geometry{
  includeheadfoot,
  margin=2.54cm
}

\makeatletter
\def\imod#1{\allowbreak\mkern10mu({\operator@font mod}\,\,#1)}
\makeatother

\newtheorem*{problem}{Problem}
\theoremstyle{definition}
\newtheorem*{answer}{Answer}

\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\f}{\varphi}
\newcommand{\e}{\epsilon}
\renewcommand{\d}{\delta}

\DeclareMathOperator{\vol}{vol}



\begin{document}

\title{Parallel Conjugate Gradients on sparse stiffness matrices}
\author{Raymond van Veneti\"e \and Jan Westerdiep}
\maketitle
\begin{figure}[h!]
	\center
  \includegraphics[scale=0.5]{Ray_surf.jpg}
   \caption{Happy, because I'm next to cartesius}
\end{figure}

\section{Introduction}
\begin{quote}
``The conjugate gradient method (CG) is an algorithm for the numerical solutions of particular systems of linear equations, namely those whose matrix is symmetric and positive definite. The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition. Large sparse systems often arise when numerically solving partial differential equations or optimization problems." \cite{wiki:cg}
\end{quote}

\begin{quote}
``The finite element method (FEM) is a numerical technique for finding approximate solutions to boundary value problems for partial differential equations. It uses subdivision of a whole problem domain into simpler parts, called finite elements, and variational methods from the calculus of variations to solve the problem by minimizing an associated error function. Analogous to the idea that connecting many tiny straight lines can approximate a larger circle, FEM encompasses methods for connecting many simple element equations over many small subdomains, named finite elements, to approximate a more complex equation over a larger domain." \cite{wiki:fem}
\end{quote}

We will be concerned with solving
\[
  Ax = b
\]
with known $A$ and $b$.

In this report, we will look at a parallel implementation of the conjuate gradient method. We will solve linear systems coming from the finite element method. Systems like this are sparse in the sense that a lot of the elements will be zero. This makes it possible to skip parts of the matrix-vector products one encounters.

\section{CG}
In its most basic (sequential, non-preconditioned) form, CG can be written down as in Algorithm~\ref{alg:seqcg}. \cite[Alg.~4.8]{biss04} We slightly adapted it from its original form to be able to use the different BLAS \cite{blas} routines.

\begin{algorithm}[H]
\caption{Sequential CG}
\label{alg:seqcg}
\SetKw{KwStep}{step}
\SetKwInOut{Input}{Input}  
\SetKwInOut{Output}{Outout}
  \Input{$k_{max} \in \N$, $\e \in \R$, $n \in \N$, $A$ symmetric $n \times n$, $b \in \R^n$}
  \Output{$x \in \R^n$ with $Ax \approx b$}
   int $k := 0$\;
   float $r[n], u[n] = \vec 0, w[n]$\;
   float $\rho, \rho_{old}, nbsq, \alpha, \beta, \gamma$\;
  
   $r \gets b$\;
   $\rho \gets \langle r, r \rangle$\;
   $nbsq \gets \rho$\;
  
	 \While{$\rho > \e^2 \cdot nbsq \wedge k < k_{max}$} {
	\uIf{$k > 0$} {
       $\beta \gets \rho/\rho_{old}$\;
       $u \gets \beta u$\;\label{line:scaleu}
		}
    
    
     $u \gets r + u$\;\label{line:axpyu}
     $w \gets Au$\;\label{line:mvw}
     $\gamma \gets \langle u, w\rangle$\;\label{line:ipgamma}
     $\alpha \gets \rho/\gamma$\;
     $x \gets x + \alpha u$\;\label{line:axpyx}
     $r \gets r - \alpha w$\;\label{line:axpyr}
     $\rho_{old} \gets \rho$\;
     $\rho \gets \langle r, r \rangle$\;\label{line:iprho}
    
     $k \gets k+1$\;
	 }
\end{algorithm}

\subsection{Sparse CG}
As FEM matrices are sparse in nature, we will adapt Algorithm~\ref{alg:seqcg} to a version that supports sparse matrices. For simplicity, we will assume the right-hand side to be dense. This allows us to effectively only change I/O and the function responsible for matrix-vector multiplication.

Our storage format uses the so-called coordinate scheme; we store tuples $(i, j, a_{ij})$ with $i$ the row number, $j$ the column number and $a_{ij}$ the matrix value at this position. The Matrix Market file format adds some headers, e.g. to denounce symmetry so that one only has to store the lower triangular part. We denote by $nz(A)$ the amount of nonzero elements in this lower triangular part. If we store the list of tuples in three lists of length $nz(A)$, namely $I$, $J$ and $v$, we can compute a sequential sparse matrix-vector multiplication using Algorithm~\ref{alg:sparsemv} (from \cite[Alg.~4.3]{biss04}).



\section{Parallellizing CG}
With our sequential algorithm in hand, we are now ready to parallellize the Conjugate gradient method. Given the symmetric positive definite matrix $A \in \R^{n \times n}$ and vector $b \in \R^n$, this requires us to find distributions for $A$, $b$ and all other vectors present in the algorithm. If we want to minimize communication cost, we desire that the distributions of $n, x, r, u, w$ are the same.\footnote{This was also pointed out in \cite[p.~174]{biss04}.} This allows us to perform all vector updates locally and makes for easy implementation of the inner product algorithm.

Every iteration of Algorithm~\ref{alg:seqcg} has 5 vector updates, two inner products and one matrix-vector multiplication. Finding good distributions of $A$, $x$ and $b$ is an NP-hard problem so we will have to resort to heuristic methods. To find these edistributions, we can use Mondriaan.\footnote{Mondriaan is a sequential program written in C that can be used to partition a rectangular sparse matrix, an input vector, and an output vector for parallel sparse matrix-vector multiplication. The program is based on a recursive bipartitioning algorithm that cuts the matrix horizontally and vertically, in a manner resembling some of the famous Mondriaan paintings. The algorithm is multilevel, hypergraph-based, and two-dimensional. It reduces the amount of communication and it spreads both computation and communication evenly over the processors. The program can partition hypergraphs with integer vertex weights and uniform hyperedge costs, but it is primarily intended as a matrix partitioner. \cite{mondriaan}} With its option \texttt{-SquareMatrix\_DistributeVectorsEqual=yes} we can force input and output vector to have the same distribution. We chose not to alter the default load imbalance option of $\epsilon = 0.03$.\footnote{Later inspection using the formula provided in \cite[p.~189]{biss04} -- $\epsilon = Vg/(2nz(A))$ being optimal for the matrix-vector product -- revealed that the optimal value (which of course depends on $p$ and $A$) lies around $0.2$. The total iteration time was however not that much faster, so we opted not to rerun all tests.}

The resulting parallel algorithm is in appearance almost exactly as Algorithm~\ref{alg:seqcg}, so we will not rewrite this. As the vector updates can be done locally without communication, BLAS routines were used (just as in the sequential algorithm). The big changes are made in computation of the inner product and the matrix-vector product. The \texttt{bspmv} algorithm found in \cite[Alg.~4.5]{biss04} was used without alterations, but the \texttt{bspip} algorithm \cite[Alg.~1.1]{biss04} was found unusable as this assumed a cyclical distribution. See Algorithm~\ref{alg:ip} for a parallel inner product algorithm that only assumes that both vectors have the same distribution. The beauty of this algorithm is that no processor has to know the distribution, as long as it has its own vector components stored as a smaller vector.

\begin{algorithm}
  \caption{Parallel inner product $\langle v, y \rangle$ assuming $v$ and $y$ have equal distributions}
  \label{alg:ip}
	\SetKw{KwStep}{step}
	\SetKwInOut{Input}{Input}  
	\SetKwInOut{Output}{Outout}
    \Input{$p$ total number of processors, $0 \leq s < p$ current processor number, $nv_s$ the amount of vector elements locally stored, $v_s,y_s \in \R^{nv_s}$ local vectors}
    \Output{$\alpha = \langle v, y \rangle$}
		float $\alpha_s = 0$\;
		float $\alpha = 0$\;
		\tcc{Compute local inner product}
		\For{$i = 0$ to $nv_s$} {
			$\alpha \gets \alpha + v_s[i] \cdot y_s[i]$
		}
		\tcc{Put local inner prodcut}
		\For{$q = 0$ to $p$} {
			Put $\alpha_s$ to $P(q)$\;
		}
		\tcc{Find global inner product}
		\For{$q = 0$ to $p$} {
			$\alpha \gets \alpha + \alpha_q$\;
		}
\end{algorithm}

\subsection{BSP cost}
We first calculate the BSP cost of the parallel algorithm per iteration, using Algorithm~\ref{alg:seqcg} as reference. Let $nv_s$ be the amount of vector elements locally stored on processor $s$. Line~\ref{line:scaleu} costs $nv_s$ operations and lines~\ref{line:axpyu}, \ref{line:axpyx} and \ref{line:axpyr} cost $2nv_s$ each. This makes vector updates contribute $7nv_s$ to the total amount of operations. 

Looking at Algorithm~\ref{alg:ip}, each inner product yields $2nv_s - 1$ operations for Superstep 1, $pg$ operations for Superstep 2 and $p-1$ operations for Superstep 3, with 2 synchronizations in between for a total of $2(2nv_s - 1 + p-1 + pg + 2l)$ inner product operations per iteration.

We found in \cite[p.~189]{biss04} that the matrix-vector product using a Mondriaan distribution yields a total BSP cost of $2(1+\epsilon)nz(A)/p + Vg/p + 4l$ operations. We have one such matrix-vector product per iteration.

Summing everything together, we get a total BSP cost per iteration of
\[
  11nv_s - 4 + p(2+2g) + (Vg + 2 (1+\epsilon)nz(A))/p + 8l.
\]

TODO wat nu? dit relateren aan het aantal iteraties dat nodig is? Wordt snel sketchy

\section{Testing our parallel CG}
To test our implementation, we want to have access to a lot of similar matrices. One way to do this is described in the book: create a random sparse matrix $B$ with values in $[-1,1]$, then take $A \gets B + B^\top + \mu I$ with $\mu$ such that $A$ is strictly diagonally dominant. As the idea of this is merely to generate a matrix $A$ that is symmetric positive definite, we opted for a slightly easier approach. 

We need to compute a \emph{symmetric} matrix, so we only have to look at the lower triangular part of this matrix. First we create a 
random sparse strictly lower triangular matrix $B$. We do this by specifying some
density $\delta$ and placing a random number in $[-1,1]$ on position $(i,j)$ with probability $\delta$. The matrix
must be positive definite, which is achieved \cite[Ex.~24.2]{trefbau} if
\[
  |A_{ii}| > \sum_{j=1, j\ne i}^n |A_{ij}|= \sum_{j=1}^{i-1} |B_{ij}| + |B_{ji}|.
\]
While placing the non-zero entries in the lower triangular part of $B$, we therefore iteratively store the sum in the right hand side of this equation for the
corresponding diagonal element. Let $\mu$ be the maximum of these sums, we now know that $B + B^\top + \mu I$ is a symmetric
positive definite matrix. Finally, to add some more randomness to the diagonal, we replace the diagonal element $\mu$ by a random number in $[\mu, \mu + 2]$ with probability $\delta$.

This parameter $\delta$ gives us an indication for the `sparseness' of the matrix.\footnote{It is actually not the density but the amount of non-zero entries per TODO row/column that is important to the complexity of the matrix-vector product \cite[p.~178]{biss04}. As we are dealing with a square matrix, these two quantities are actually equivalent.} With this method we can generate a variety of matrices
for different combinations of $n$ and $\delta$, which gives insight in the performance of our parallel implementation.

\subsection{Benchmark of Cartesius}
\label{sec:cart}
We ran \texttt{bspbench} with $n=100$, $h=256$ and $p \in \{1, \ldots, 64\}$ to get an idea of values for $r$, $g$ and $l$ for processor counts and get an idea of the scaling properties of Cartesius.

We found the average value of $\bar r = 9157$ Mflop/s. See Figure~\ref{fig:cart} for values of $g$ and $l$ as function of $p$. We see a sharp increase in both $g$ and $l$ at $p=24$ and another one at $p=32$. These have to do with the hardware of Cartesius: under 24 cores is done on a shared-memory system, where communication is cheap. At $p=32$, TODO waarom is dit hoger dan?

\begin{figure}
  \includegraphics[width=0.49\linewidth]{cartg.png}
  \includegraphics[width=0.49\linewidth]{cartl.png}
  \caption{Results of running \texttt{bsbench} on a thin node of Cartesius with $n=100$, $h=256$. $x$-axis: $p \in \{1, \ldots, 64\}$. $y$-axis: left -- $g$, right -- $l$.}
  \label{fig:cart}
\end{figure}

\subsection{Scaling tests}
With these matrices in hand, we can perform scaling tests by varying 
\[ p \in \{1, 2, 4, \ldots, 64\}, \quad n \in \{64, 128, \ldots, 4096 \}, \quad \delta \in \{0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2\} \]
for slightly less than\footnote{Some matrices created errors. For instance $n = 64, \delta = 0.01$ creates on average $40.96$ non-zeroes. Trying to call Mondriaan with $p=64$ on this matrix generates an error.} $7 \cdot 7 \cdot 7 = 343$ matrices and for each matrix, approximated the solution $x$ to $Ax=b$ with $b$ equal to
\begin{enumerate}
  \item $b = \vec 1 = (1, \ldots, 1)$ (easy right-hand side);
  \item $b = A\vec 1$ (known solution);
  \item $b_i \in [0, 1]$ random (random right-hand side).
\end{enumerate}

For each system, we computed:
\begin{gather*} 
P, n, nz(A), \delta = nz(A)/n^2, k \text{(number of iterations)}, \\
t_i~ \text{(time to initialize)}, \quad t_{mv}~ \text{(time matrix-vector)}, \quad t_{ip}~ \text{(time inner product)}, \\
t_l~ \text{(time local operations)}, \quad t_g~ \text{(time spent computing global solution)}, \\
t_c = t_{mv} + t_{ip} + t_l~ \text{(time spent in while-loop)}, \quad t = t_i + t_{mv} + t_{ip} + t_l + t_g~ \text{(total time)}.
\end{gather*}

The first thing we saw is that the choice of RHS is hardly important for the computation time $t_c$. We therefore made the arbitrary choice to use $b = \vec 1$.

We quickly realized that time spent doing local operations $t_l$ and computing global solution $t_g$ will probably not be of much significance. This was quickly verified: see Figure~\ref{fig:nonsig}.
\begin{figure}
  \includegraphics[width=0.49\linewidth]{tlt.png}
  \includegraphics[width=0.49\linewidth]{tgt.png}
  \caption{Percentage of total time spent doing local computations (left) and finding the global solution from the local components (right) for our test matrices.}
  \label{fig:nonsig}
\end{figure}

One other thing we noticed right away is the fact that for our class of matrices $A = B + B^\top + \mu I$, $k$ is very low (in the order of 10). This has the effect that $t_i$ greatly overshadows $t_c$ for large $nz(A)$: see Figure~\ref{fig:quotient}. The reason for this is very insightful to the problem: while strict diagonal dominance implies positive definiteness, it also implies that the diagonal elements are \emph{much} larger than the off-diagonal elements. The resulting matrix is close to being diagonal! As solving $Ax=b$ for a diagonal matrix using CG completes in one step, it is no surprise that CG ends quickly when $A$ is near-diagonal.

\begin{figure}
  \includegraphics[width=\linewidth]{quotient.png}
  \caption{Quotient of initialization time $t_i$ and computation time $t_c$ for random matrices $A$ as function of $nz(A)$.}
  \label{fig:quotient}
\end{figure}

Taking $n \in \{2048, 4096\}$ and $\delta \in \{0.1, 0.2\}$, we get a few sample matrices. Looking at $t_{mv}$ and $t_{ip}$ for these four matrices yields Figure~\ref{fig:samples}. We see that the optimal computation time ($t_c = t_l + t_{mv} + t_{ip} \approx t_{mv} + t_{ip}$ as $t_l \approx 0$ by Figure~\ref{fig:nonsig}) seems to lie somewhere between $p=8$ and $p=32$. The most likely option is around $p=24$, as this is the highest amount of processor cores that is still on one shared-memory system (see \S \ref{sec:cart}). We can overcome this by increasing the computational effort every processor has to do (increasing $n$ and $nz(A)$), but this proved hard, as Mondriaan takes quite some time to generate the distributions already.

If we look at $t_{mv}$ and $t_{ip}$ separately, we see that for $p=1$, $t_{mv}$ scales linearly with $nz(A)$ (as we concluded before). We also see that for $p=64$, $t_{ip}$ is almost the same across the different matrices. This indicates that the time spent computing the local inner product is heavily outweighed by the communication costs. This raises the question if $t_{ip}$ would improve had we used the inner product communication strategy from the first homework exercise. \cite{TODOHUISWERK} In this exercise, we found that if
\[
  (p - 5\log_2 p) + (p + \log_2 p - 1)g + (2-\log_2 p)l > 0
\]
the new strategy is better. A quick inspection yields that this is true for $p \leq 4$ and \emph{very} false (values being in the order of negative millions) for $p > 4$, indicating that we won't have to try.

\begin{figure}
  \includegraphics[width=0.48\linewidth]{n2048d0_1mvip.png}
  \includegraphics[width=0.48\linewidth]{n2048d0_2mvip.png}
  \includegraphics[width=0.48\linewidth]{n4096d0_1mvip.png}
  \includegraphics[width=0.48\linewidth]{n4096d0_2mvip.png}
  \caption{Timing results for various sample matrices. Purple: $t_{ip}$; green: $t_{mv}$. Top row: $\delta = 0.1$. Bottom row: $\delta = 0.2$. Left column: $n = 2048$. Right column: $n = 4096$.}
  \label{fig:samples}
\end{figure}

\section{Creating the FEM-mesh}
In two dimensions, different domains $\Omega$ provide vastly different problems of the form \eqref{eqn:problem}. We chose to generate a mesh using \cite{TODOJAN}. To find a partitioning for this mesh, we used the Mondriaan hypergraph partitioning tool.

First, we create some mesh and save it to disk in some file format. We then transform this into a hypergraph, which we store in the coordinate scheme which Mondriaan can import. We then run Mondriaan on this file, receiving a triangle distribution. We load this file into a distributed mesh and save this mesh to file.

The most theoretically interesting step is converting a mesh to a hypergraph. We have a set of vertex coordinates $V = \{v_1, \ldots, v_{nverts}\}$ with $v = (x, y)$ and a set of triangles $T = \{ t_1, \ldots, t_{ntris}\}$ with each triangle expressed in its vertices: $t = (v_{i_1}, v_{i_2}, v_{i_3})$. Then we have a list $B = (b_1, \ldots, b_{nverts})$ denoting if vertex $v_i$ is on the boundary $b_i = 1$ or not $b_i = 0$.

A hypergraph is a generalization of a graph where hyperedges (also called \emph{nets}) are allowed to connect any number of hypervertices. We will identify triangles in the mesh with hypervertices, and vertices in the mesh with nets. A net connects triangles iff its corresponding vertex is a vertex of all these triangles. TODO beter uitleggen.

\section{FEM and CG}
We are now trying to solve a FEM system using CG. More specifically, we want to find a (numerical) solution to the two-dimensional PDE
\begin{equation}
  \label{eqn:fem}
  \begin{cases} -\Delta u = f & \text{ in } \Omega \\ u = 0 & \text{ on } \partial \Omega \end{cases}
\end{equation}
with $\Omega$ a polygonal domain. In this report, we will look at $f=1$, further narrowing the problem.

The idea of the Finite Element Method is to find the projection of $u \in C^2_0(\Omega)$ -- twice continuously differentiable functions that are zero on the boundary of $\Omega$ -- onto some (finite dimensional) linear subspace $V$. We will take $V$ to be the space of piecewise linear functions, subject to some partitioning of $\Omega$ into \emph{elements}. Often, these elements will be triangular. Choosing $\Omega$ to be polygonal allows us to triangulate $\Omega$ into triangles $T = \{ T_0, \ldots T_{N-1}\}$ such that $\Omega = \cup_{k = 0}^{N-1}T_k$. We will denote the vertices of these triangles with $X = \{x_0, \ldots, x_{n-1}\}$ and order them such that $x_0, \ldots, x_{D-1}$ lie in $\Omega$ with $x_{D}, \ldots, x_{n-1}$ on $\partial \Omega$.

This triangulation cannot be arbitrary, as this introduces \emph{hanging nodes} and other nasty side-effects. We assume triangulations to be \emph{conforming}; in this report, this means that ``everything works nicely''.

The fact that functions in $V$ are zero on the boundary implies that $\dim(V) = D$. The number $D$ is also called the \emph{degrees of freedom}(DOF) for a given triangulation. A useful basis for this subspace $V$ is the \emph{nodal} basis $\Phi = \{\phi_i: 0 \leq i < D\}$ of hat functions uniquely determined by the property
\[
  \phi_i( x_j) = \delta_{ij} \quad \forall~~0 \leq i < D,\, 0 \leq j < N.
\] An example of such a basis function is given in Figure~\ref{fig:nodal}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{nodal_vijfhoek.eps}
\caption{Example of a basis function for a pentagon}
\label{fig:nodal}
\end{figure}


The crux of the Finite Element method is that we can actually find the projection $u_V$ of $u$ into $V$ \emph{without knowing the exact solution $u$}! Write $u_V$ in the nodal basis:
\[
  u_V = \sum_{i=0}^{D-1} \alpha_i \phi_i.
\]
Then, $u_V$ can be proven \cite{TODO} to be the solution to the system
\[
	A\vec \alpha = b \text{ with }  A_{ij} := a(\phi_i, \phi_j) = \int_\Omega \nabla \phi_i \cdot \nabla \phi_j \text{ and } b_i = \int_\Omega \phi_i.
\]

The matrix $A$ is called the \emph{stiffness matrix}, which is \cite{TODORAYMOND} a symmetric positive definite matrix. It is also a sparse matrix, as the basis functions only `interact' with a small number of neighbours.

In the coming sections, we will look at the two steps involved in solving such a FEM-system:
\begin{description}
  \item[Assembly] is the act of finding the nonzero entries of $A$ and $b$;
  \item[Solving] the system; as $n$ (and thus $D$) is generally very large, we want to use an efficient solver. We will be using CG.
\end{description}

\subsection{System assembly}
How can we find the elements of the stiffness matrix in an efficient manner? In other words, how do we calculate $a(\phi_i, \phi_j)$? The following elegant derivation uses the fact that the domain can be decomposed into triangles:
\begin{equation}
  \label{eqn:div}
	a(\phi_i, \phi_j) = \int_\Omega \nabla \phi_i \cdot \nabla \phi_j = \int_{T_0 \cup \dots \cup T_{N-1}} \nabla \phi_i \cdot \nabla \phi_j = \sum_{n = 0}^{N-1} \int_{T_n} \nabla \phi_i \cdot \nabla \phi_j =: \sum_{n=0}^{N-1}a_{T_n}(\phi_i, \phi_j).
\end{equation}

We will call the matrix $A_T$ with $(A_T)_{ij} = a_T(\phi_i, \phi_j)$ the \emph{element matrix} of triangle $T$ containing information on interactions of basis functions restricted to $T$. This reduces the problem of finding the stiffness matrix to finding the element matrices. Note that these matrices are very sparse: in fact, they only have $3 \times 3$ nonzeros as there are only 3 basis functions that have interactions per triangle. Therefore, we will compute a $3 \times 3$ matrix $\hat A_T$ and scale this up to the $D \times D$ element matrix $A_T$.


In fact, this $3 \times 3$ matrix can be computed in a variety of ways. One way is to use
\[
  D = \begin{bmatrix} v_{0} & v_{1} & v_{2} \end{bmatrix} \in \R^{ 2 \times 3}
\]
with $v_{i}$ the $i$th vertex of triangle $T$. Then one can prove \cite{TODO} that
\[
  \hat A_T = \frac{D^\top D}{4 \cdot \text{vol}(T)}
\]
with $\text{vol}(T)$ the volume of triangle $T$.

We can compute the right-hand side $b_i$ in much the same way. One can proof that
\[
  b_i = \int_\Omega \phi_i = \sum_{n=0}^N \int_{T_n} \phi_i
	= \sum_{n=0}^N \mathbf{1}_{T_n}(x_i)\text{vol}(T_n)/3.
\]

With these notes we can easily derive Algorithm~\ref{alg:calc_fem} which constructs a FEM-system for a given triangulation.

\begin{algorithm}[H]
\SetKw{KwStep}{step}
\SetKwInOut{Input}{Input}  
\SetKwInOut{Output}{Outout}
\Input{$X : $ list of vertices, \\
       $T : $ list of $N$ triangles, \\
		   $D : $  degrees of freedom.}
\Output{$ A : $ sparse $d\times d$ FEM-matrix , \\
	      $ b : $ vector of length $D$, the rhs of the system.}
	\tcc{Initalize to zero}
	$b := 0$\;
	$A := 0$\;
	\For{$k : 0 \leq k < N$} {
		Generate element matrix $\hat A_{T_k}$\;
		\tcc{Loop over indices in the element matrix}
		\For{$li : 0\leq li < 3$} {
			Calculate global index $i$ corresponding to $li$\;
			\tcc{Check if vertex $li$ corresponds to a DOF}
			\uIf{$i < D$} {
				$b_i = b_i + \text{vol}(T_k)/3$\;
				\For{$lj : 0 \leq lj < 3$} {
					Calculate global index $j$ corresponding to $lj$\;
					\uIf{$ j < D$} {
						$A_{i,j} := A_{i,j} + (A_{T_k})_{li,lj}$\;
					}
				}
			}
		}
	}
 \caption{Calculate the FEM-matrix.}
 \label{alg:calc_fem}
\end{algorithm}
\subsection{Solving the system}
In the previous section we saw how to assemble the FEM system. One could use this approach to create a `naive' parallel FEM solver: let one processor assemble the system, find a parallel distribution (using e.g. Mondriaan) and apply the parallel CG algorithm described in Section\cite{BLA}. This has a few disadvantages:
\begin{enumerate}
  \item Matrix assembly is computationally expensive;
  \item This system is possibly enormous, limiting us to systems that fit in memory of a single processor;
  \item The relation between the geometrical object, triangulation and nonzeros is lost: $a_{ij}$ is nonzero iff $\phi_i$ and $\phi_j$ interact, which only happens if $x_i$ and $x_j$ are connected by an edge.
\end{enumerate}

We will look at a smarter approach which solves these disadvantages. Instead of distributing the matrix nonzeros $A_{ij}$ between processors, we distribute the triangles $T_n$ over $p$ processors, thereby solving disadvantage (3). We divide our triangulation into $p$ disjoint sets of length $N_q$
\[
  T = \bigsqcup_{q=0}^{p-1} T^q, \quad T^q = \{ T^q_0, \ldots, T^q_{N_q-1} \}
\]
leading to the partition $\Omega = T^0 \cup \dots \cup T^{p-1}$.

A side-effect of this is that a vertex $x_i$ can belong to multiple processors. Therefore we define its \emph{processor set} 
\[
  \Pi: \{0, \ldots, D-1\} \to 2^{\{0, \ldots, p-1\}}: i \mapsto \Pi(i)
\]
as the set of processors that contain $x_i$ in one of their triangles.

Using \eqref{eqn:div} leads to a natural decomposition of $A$ into
\[
	A = \sum_{q=0}^{p-1} A^q\quad \text{ with } A^q= \sum_{n=0}^{N_q-1} A_{T^q_n} \quad \text{ so that } (A^q)_{ij} = \sum_{n=0}^{N_q-1} a_{T_n}(\phi_i,\phi_j).
\]
These matrices $A^q$ are $D \times D$ but again very sparse. We identify $A^q$ with a smaller matrix $\hat A^q$ of size $D_q \times D_q$ by removing zero rows and columns. As CG requires the calculation of $v = Au$, we will first calculate $v^q = A^q u$ (which is in practice done by computing the smaller system $\hat v^q = \hat A^q \hat u$). Now to find the final solution we need to calculate $v = \sum_{q =0}^{p-1} v^q$. 
Every non-zero element in $v^q$ corresponds to a vertex $x_i$ \footnote{Actually to a degree of freedom, but in our case degrees of freedom correspond to vertices inside the domain}, which is shared by the processors in its processor set $\Pi(i)$. To ensure that every processor in $\Pi(i)$ has the correct value of $v_i$ we must communicate the value of this nonzero in $v^q$ to all the processors in $\Pi(i)$. In Algorithm~\ref{alg:fem_mv} we give the implementation of this parallel $v = Au$ for this FEM-matrix.
\begin{algorithm}[H]
\SetKw{KwStep}{step}
\SetKwInOut{Input}{Input}  
\Input{$A^s: $ sparse (local) FEM-matrix for $P(s)$, such that
	     $A = \sum_{t=0}^{p-1} A^t$, \\
       $\Pi : \{0, \dots, D-1\} \to 2^p$, \\
			 $v : $ vector of length $D$, $\text{distr}(v) = \Pi$.}
\KwOut{$u : Av$, $\text{distr}(u) = \Pi$}
	\nlset{(0)}
	Local sparse symmetrical matrix-vector $u_s = A^s v$ by $\hat u_s = \hat A^s \hat v$\;
	\nlset{(1)}
	\For{$ i : 0 \leq i < D \wedge s \in \Pi(i)$} {
		\For{$t \in \Pi(i)$} {
			put $(u_s)_i$ in $P(t)$\;
		}
	}
  
  \nlset{(2)}
	\For{$ i : 0 \leq i < D \wedge s \in \Pi(i)$} {
		$u_i := 0$\;
		\For{$t \in \Pi(i)$} {
			$u_i := u_i + (u_t)_i$\;
		}
	}
 \caption{Matrix-vector product for a FEM-system for $P(s)$}
 \label{alg:mv_fem}
\end{algorithm}

To create $\hat A^q$ we need to identify the zero rows (and by symmetri columns) of $A^q$. A row $i$ is zero if $(A^q)_{ij} = 0$ for $0 \leq j < D$, which corresponds to the situation where basis function $\phi_i$ has no interactions on any of the triangles contained by processor $q$. Translating this back to the triangulation, we see that this happens if processor $q \not \in \Pi(i)$. This gives us an explicit formula for $D_q$:
\[
	D_q = \#\{ i | 0 \leq i < D,\, q \in \Pi(i)\}.
\]

Algorithm~\ref{alg:init_fem} describes how to create all the data needed for executing our parallel $Av$ with the stiffness matrix $A$.
\begin{algorithm}[H]
\SetKw{KwStep}{step}
\SetKwInOut{Input}{Input}  
\SetKwInOut{Output}{Outout}
\Input{$X :$ list of $n$ vertices, with first $D$ vertices corresponding to a DOF, \\
	     $T : $ list of $N$ triangles, $\text{distr}(T) = \psi$.}
\Output{$\Pi:$ proccesor set for each DOF, \\
	     $\Pi_{\text{owner}}:$ the `owner' of a DOF, \\
			 $A^s: $ local sparse FEM-matrix for $P(s)$, \\
			 $b : $ vector of length $D$, $\text{distr}(b) = \Pi$.}
  \nlset{(0)}
	\If{$ s = 0$} {
		Use $\phi$ to calculate $\Pi$, the processor set for each DOF\; 
		Put the local amount of DOF, vertices and triangles in each processor\;
	}
  \nlset{(1)}
	Allocate memory for the (local) FEM data\;
	\nlset{(2)}
	\If{$ s= 0$} {
		\For{$i : 0 \leq i < N$} {
			put $T(i)$ in $P(\psi(i))$\;
			put vertices of $T(i)$ in $P(\psi(i))$ \tcc*[h]{Avoid duplicates in implementation}
		}
	}
  \nlset{(3)}
	Calculate local FEM-matrix $A^s$ and local right hand side $b^s$\;
	
  { Produce the entire vector $b$, same method as described in Alg~\ref{alg:mv_fem} }
	\nlset{(4)}
	Communicate entries of $b^s$ to processor sets, see superstep $(1)$ in Alg~\ref{alg:mv_fem}\;
	\nlset{(5)}
	Sum the values to find $b$, see superstep $(2)$ in Alg~\ref{alg:mv_fem}\;
 \caption{Algorithm that calculates the local FEM data.}
 \label{alg:init_fem}
\end{algorithm}

Note that the CG algorithm also requires us to calculate the inner product. 
\begin{algorithm}[H]
\SetKw{KwStep}{step}
\SetKwInOut{Input}{Input}  
\Input{$x,y : $ vector of length $D$, \\
	     $\Pi_{\text{owner}} : \{0, \dots, D-1\} \to P$,\\
			 $\text{distr}(x) = \text{distr}(y) = \Pi_{\text{owner}}.$}
\KwOut{$a  := \langle x, y\rangle$}
\nlset{(0)}
  $a_s := 0$\;
	\For{$ i: 0 \leq i < D \wedge \Pi_{\text{owner}}(i) = s$} {
	  $a_s := a_s + x_i y_i$\;
	}
\nlset{(1)}
  \For{$ t: 0 \leq t < p$} {
		put $a_s$ in $P(t)$\;
	}
\nlset{(2)}
  $a := 0$\;
  \For{$ t: 0 \leq t < p$} {
		$a := a + a_t$\;
	}
 \caption{Inner product for vectors in FEM-system for $P(s)$}
 \label{alg:ip_fem}
\end{algorithm}
\subsection{Finding a processor distribution}
Given a vertex $x_i$, there are two possibilities:
\begin{enumerate}
  \item $|\Pi(i)| = 1$. This means that all interactions of the basis function $\phi_i$ are stored on a single processor $q$, meaning that $(Au)_i = (A^qu)_i$ or in other words, no communication is necessary. This corresponds with $x_0$ in Figure~\ref{fig:procset};
  \item $|\Pi(i)| > 1$. Interactions of $\phi_i$ are stored on different processors. To compute $(Au)_i$, we need to communicate with $|\Pi(i)|-1$ processors. This corresponds with vertices $x_1, x_2$ in Figure~\ref{fig:procset}.
\end{enumerate}

\begin{figure}
  \includegraphics[width=0.5\linewidth]{procset.png}
  \caption{A sample mesh with its processor distribution. Only vertices not on the boundary have been numbered. For these vertices, $\Pi(0) = \{0, 1, 2, 3\}, \Pi(1) = \{0\}, \Pi(2) = \{0, 1\}$.}
  \label{fig:procset}
\end{figure}

Here we see the second advantage of this approach. By distributing triangles, we can balance computation and communication cost of a matrix-vector multiplication. We chose to use Mondriaan, using \cite{bissmondriaan,biss2012}. First, we create some triangulation (e.g., using \cite{TODOJAN}) and save this to disk for later retrieval. We then transform this into a \emph{hypergraph} $H = (V,E)$ -- a generalization of a graph in which any number of hypervertices can be connected by hyperedges or \emph{nets}. This is done as follows:
\begin{itemize}
  \item[-] Every triangle $T_n$ in our triangulation is identified with a hypervertex $v_n \in V$;
  \item[-] Every vertex $x_i$ connecting triangles $\{T_{n_0}, \ldots, T_{n_k}\}$ is identified with a net $e_i \in E$ connecting hypervertices $\{v_{n_0}, \ldots, v_{n_k}\}$.
  \item[-] Every hypervertex $v_n \in V$ is given a \emph{vertex weight}, measuring the computational cost corresponding with triangle $T_n$. In our case, one needs to compute the same quantities for every triangle.\footnote{In more advanced Finite Element methods where the polynomial degree is different for each triangle, this would scale with the polynomial degree.} We set this to 1;
  \item[-] Every net $e_i\in V$ is given a \emph{net weight}. Mondriaan does not implement this, but to be future-proof, we set this to 1 for all vertices $v_i$ not on the boundary and 0 for boundary vertices. This reflects the fact that boundary vertices have no degree of freedom attached to it.
\end{itemize}
Mondriaan then aims to balance communication and computation cost between processors by creating a distribution of the hypervertices. It requires setting the amount of processors $p$ and a load imbalance factor $\epsilon$ which we set to $0.1$.



\subsection{Implementation details}
\subsubsection{FEM Matrix}
Alg~\ref{alg:calc_fem} gives an algorithm to assemble the FEM Matrix. Here
we assumed to have a method for inserting or adding nonzeros at abritrare positions in the matrix. However,
this is not easily done in most data structures for sparse matrices as we do not know the
indices holding a non-zero beforehand. We adapt the following strategy, we first create a list
of all the nonzeros to add together in the triple format $(i,j,a_{ij})$.
This list will contain multiple elements with the same index $ij$ when
the basis function $i$ and $j$ interact on multiple triangles. Afterwards we know
exactly what indices contain a nonzero. We then convert this triple format to the \emph{ICRS}
\cite[p.~171]{biss04} format, summing the values for the same indices.

As the matrix is symmetric we only store the lower triangular part. In Alg~\ref{alg:mv_fem}
we calculate the matrix-vector product of the stiffness matrix and a vector. Algorithms for this
follow easily from the ICRS data structure. We implemented this like \cite[Algorithm~4.4]{biss04}, with
a slight alteration to cope with symmetric matrices.

\subsubsection{Indexing}
In Alg~\ref{alg_mv_fem} we have to communicate values corresponding to a shared vertex. Here we have an indexing problem, as each (shared) processor has a different index for the vertex. We solve this by using the orginal, or \emph{global} index. This is the index of the vertex in the initial triangulation. Each processor uses \verb=mpi_send= with the global index as tag to send the value to the other processors. Furthermore, each processor gets an array \verb=global2local= in the init stage that converts a global vertex index to a local one.


In superstep (1) of Alg~\ref{alg:mv_fem} we do not have to communicate $(u_s)_i$ values if $\Pi(i) = s$. To do this efficiently we have chosen to first store the shared vertices and then stored the vertices for which we have $\Pi(i)=s$. As processor s is the only processor we do not need to store the entire processor set.
\section{Future work}
\subsection{Preconditioned CG}
The iterates $x_k$ obtained from the CG algorithm satisfy the following inequality \cite[Lect.~7]{sleij}:
\[
  \frac{\|x - x_k\|_A}{\|x - x_0\|_A} \leq 2 \left( \frac{ \sqrt{\kappa_2(A)}-1}{\sqrt{\kappa_2(A)}+1}\right)^k \leq 2 \exp \left( -\frac{2k}{\sqrt{\kappa_2(A)}}\right) 
\]
where $\kappa_2(A)$ is the $2$-condition number of $A$, which for symmetric positive definite matrices equates
\[
  \kappa_2(A) = \frac{\lambda_{max}}{\lambda_{min}}.
\]
It is therefore of interest to create a condition number that is as low as possible.

A preconditioner $P$ of a matrix $A$ is a matrix such that $P^{-1}A$ has a smaller condition number than $A$. As the theoretical convergence rate is highly dependent on the condition number, we can improve this using such a preconditioner. Instead of solving $Ax = b$, we will solve $P^{-1}Ax = P^{-1}b$. This preconditioner should satisy:
\begin{itemize}
  \item Convergence time should be faster for the preconditioned system. Normally, this means that $P$ is constructed as an ``easily invertible'' approximation to $A$;
  \item Operations with $P^{-1}$ should be easy to perform;
  \item $P$ should be (relatively) easy to construct.
\end{itemize}

TODO uitleggen wat implicit preconditioning precies inhoudt

Algorithm~\ref{alg:pcg} shows CG with implicit preconditioning. \cite[Lect.~10]{sleij}
\begin{algorithm}
  \caption{CG with implicit preconditioning \cite[Lect.~10]{sleij}}
  \label{alg:pcg}
	\SetKw{KwStep}{step}
	\SetKwInOut{Input}{Input}  
	\SetKwInOut{Output}{Outout}
    $\rho \gets 1$;
		\While{asdf} {
 \tcc{Solve $Pc = r$ for c}
       $c \gets P^{-1} r$\;
       $\rho_{old} \gets \rho$\;
       $\rho \gets \langle c, r \rangle$\;
      
			 \uIf{$k > 0$} {
         $\beta \gets \rho/\rho_{old}$\;
         $u \gets \beta u$\;
			}
       $u \gets c + u$\;
       $w \gets Au$\;
       $\gamma \gets \langle u, w \rangle$\;
       $\alpha \gets \rho/\gamma$\;
       $r \gets r - \alpha w$\;
       $x \gets x + \alpha u$\;
		 }
\end{algorithm}

\subsection{Choice of preconditioner}
If $A$ is positive definite, the diagonal tells us a lot about the properties of $A$ so it makes a certain amount of sense to consider perhaps the simplest preconditioner of all:
\[
  P = \text{diag}(a_{11}, \ldots, a_{nn}).
\]
The beauty of this preconditioner is the fact that its sparsity pattern is just like a dense vector. Therefore, given a vector distribution (like we already have in our implementation), creating a preconditioned version is easy. We did not have time to implement this.

A more advanced preconditioner that is often used in the symmetric positive definite case is Incomplete Cholesky.  The Cholesky factorization of a positive definite matrix $A$ is $A = LL^*$ with $L$ a lower triangular matrix. An incomplete Cholesky factorization is any sparse lower triangular $K$ that is in some sense close to $L$. One such $K$ can be found by finding the exact Cholesky decomposition, except that any entry is set to zero if the corresponding entry in $A$ is also zero. \cite[\S11.5.8]{golub}
\subsection{Adaptive FEM}
\subsection{PaToH ipv Mondriaan}


\bibliographystyle{alpha}
\bibliography{report}

\end{document}
